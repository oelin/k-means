"""Implementation of the k-means clustering algorithm.

k-means clustering is a method of vector quantization that aims to partition
n observations into k <= n clusters, where each observation's value is
closer to the mean of its cluster than to the means of other clusters.

The means of each cluster act as cluster prototypes.

k-means clustering is np-hard however heuristics that converge to local optima
are viable compromises.

Problem Formulation
-------------------

Given a set of d-dimensional observations (x1, ..., xn), we wish to find a
partition of the observations such that for each subset (cluster), ci the
*variance is minimal*. This is, we wish to minimize the following loss
function w.r.t. the partition P:

    L(P) = sum[i to k] sum[x in ci] ||x - ui||^2
         = sum[i to k] |ci| Var[ci]

where ui is the mean of cluster ci, given by

    ui = 1/|ci| sum[x in ci] x.

It can be shown that minimizing L(P) is equivalent to minimizing the
pairwise distances between points in each cluster.

Algorithms
----------

There are a variety of heuristic algorithms for computing locally optimal
solutions to the k-means problem.

1. Naive k-means

The naive k-means is an effective but inefficient k-means algorithm, and
one of the most used in practice.

The algorithm uses an interative appraoch to minimize the loss function.
It proceeds in two simple steps, given an initial set of k means:

(1) Assigment: assign each observation to the cluster with the nearest
               mean.

(2) Update: recompute the means of each cluster.

Then rinse and repeat until the assignments no longer change.


Choosing initial means:
* Random partition: create a random initial partition and use the
                    corresponding means as the initial k means.

* Forgy: randomly select k observations from the dataset and use
                  their values as the k means.

Which method is best depends on the specific application/variant of
clustering in use. The key point is that while forgy tends to
spread out the means, random partition tends to lead to similar
means, since each random cluster will have similar means to the
mean of the whole dataset.
"""

import random
import numpy as np


def naive_k_means(observations: np.array, k: int, iterations: int = 10) -> np.array:
    """Implements the naive k-means algorithm with Forgy
    initialization.
    """

    clusters = None
    means = np.array([random.choice(observations) for _ in range(k)])

    for i in range(iterations):
        clusters = [[] for _ in range(k)]

        for observation in observations:
            distances = [np.linalg.norm([mean - observation]) for mean in means] # Euclidean distance to each mean.
            closest  = np.argmin(distances)
            clusters[closest].append(observation)  # Assignment step


        # Update step: recompute means.

        means = np.array([np.mean(cluster, axis=0) for cluster in clusters])

    return [np.array(cluster) for cluster in clusters]


data = np.array([[ 0.53363661,  1.07439638],
       [-0.60523989, -0.31253801],
       [ 0.60934684, -0.63268314],
       [ 0.26199728, -0.45816219],
       [-2.35852925, -0.8500485 ],
       [-0.12238977, -1.31775553],
       [-0.98563877,  0.68374145],
       [ 0.41329912,  0.60398521],
       [ 2.96872281,  0.06184291],
       [-1.50394916,  1.86156214],
       [ 5.70309587,  5.54994829],
       [ 5.47727296,  5.38051453],
       [ 4.97086361,  5.958189  ],
       [ 4.42894221,  3.8464779 ],
       [ 5.22281781,  4.37080504],
       [ 5.17782114,  5.1238812 ],
       [ 4.79134295,  5.20056089],
       [ 3.70291956,  4.28637125],
       [ 3.75776455,  3.66749893],
       [ 4.02699607,  5.71496063],
       [-7.48580101,  7.92901896],
       [-4.28161874,  7.30723928],
       [-6.90203276,  8.97565222],
       [-6.66439406,  7.39901078],
       [-6.71762093,  7.52424908]])

# Generated by p(x|k) where:
# k ~ {0: 10/25 1: 10/25, 2: 5/25}
# x|k=0 ~ N([0,0], 1)
# x|k=1 ~ N([5, 5], 1)
# x|k=2 ~ N([-6, 8], 1)


clusters = naive_kmeans(data, k=3)

print(clusters[0].shape, clusters[0].mean(axis=0))
print(clusters[1].shape, clusters[1].mean(axis=0))
print(clusters[2].shape, clusters[2].mean(axis=0))

# We see that kmeans successfully recovered each of the
# underlying clusters, matching both their size and means
# accurately.
